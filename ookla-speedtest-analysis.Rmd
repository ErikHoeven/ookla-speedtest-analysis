---
title: "Ookla Speedtest Analysis"
author: "Hugo Koopmans"
date: "07-01-2015"
output:
  pdf_document:
    fig_height: 4
    keep_tex: yes
    toc: yes
---

\newpage

Colofon
-------------
this analysis is performed by DIKW Consulting.

For questions contact Hugo Koopmans at
hugo.koopmans@dikw.com or
+31 6 43106780

Code generation
---------------
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

\newpage

Introduction
------------
In this document we have conducted a statistical analysis of Ookla Speedtest data. The time period we consider is Q4 2014 so the months October, November and December of 2014. We perform an analysis of [Ookla](http://www.ookla.com/) speedtest data on three different measures: download speed, upload speed and latency(or ping). On these metrics we compare the three major providers of 4G mobile networks in the Netherlands, Vodafone, KPN and T-Mobile.

T-Mobile has asked DIKW Consulting to perform this test as an independent third party. DIKW Consulting was payed to perform this test by T-mobile and has no other intentions then to perform this test by it's own high quality standards. The analysis was performed by generally accepted and approved standards and statistical methods using open source tools.

We let the data speak for itself.

If you have questions you can contact [DIKW Consulting](http://www.dikw.nl). If you want to repeat this test by yourself you are welcome to do so, all necessary scripts are available on GitHub. The data is commercially available at [Ookla](http://www.ookla.com/).

This analysis, method, tools and scripts are open sourced and placed on GitHub, see the readme on the github [repository](https://github.com/hugokoopmans/ookla-speedtest-analysis).

Data
======
For this analysis we use a postgres database that is loaclly installed.

The data is loaded from three different files, each file resembling a mobile platform(Andriod,iPhone and Windows mobile). The data is loaded asis as it was recieved from the Ookla server. Scripts to load this data directly into postgresql db can be found in the gitbub repository.

All scripts to process the data are in SQL(available on github) or R(included in this document, thanks to [knitr](http://yihui.name/knitr/))

```{r options , echo=FALSE, cache=FALSE}

# set global chunk options 
library(knitr)
opts_chunk$set(echo=FALSE, cache=FALSE, tidy=TRUE, warning=FALSE, message=FALSE, error=TRUE)

# load functions
source("statistical-analysis.R")

```


Let's get the data and do some basic stats.

```{r get-raw-data}

# postgreSQL
require("RPostgreSQL", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.0")

# Establish connection to PoststgreSQL using RPostgreSQL
drv <- dbDriver("PostgreSQL")

# read data from 4Gdb
# Full version of connection setting
con <- dbConnect(drv, dbname="4Gdb",host="localhost",port=5432,user="4Guser",password="4Gpassword" )

# all data joined
df.ad <- dbReadTable(con, c("ookla_all_data"))

# disconnect
dbDisconnect(con)
```

The raw data
---------------
From the raw files downloaded from the Ookla server per device we load these into individual tables. Then we merge these into one table. We have `r nrow(df.ad)` rows in the joined table.
```{r table}
library(knitr)
tbl <- table(df.ad$os)
kable(as.data.frame(cbind(tbl,100*prop.table(tbl))),col.names = c('count','percentage'), digits = 2,caption="Raw test data counts")
```
Windows mobile devices have limited representation in the dataset.

Data Analyses
---------------
To understand the data better we do some analysis and we look at suspicious artifacts that coould influence results. typically we will look for device-id's that are used too often. Locations(specific latitude-longitude coordinates) that occur to often. We will look at the timeseries to see if there is a peak or pattern. We check if there are measurements that exceed the theoretical maximum speeds for the technology used.

### Suspicious devices

Are there any devices that perform tests very frequently? 

Let's look at a frequency plot of devices that occure more then ten times in each month.

```{r freq-plot-devices}
library(ggplot2)

# months
df.ad.oct <- df.ad[as.Date(df.ad$test_date) >= as.Date('2014-10-01') & as.Date(df.ad$test_date) < as.Date('2014-11-01'),]
df.ad.nov <- df.ad[as.Date(df.ad$test_date) >= as.Date('2014-11-01') & as.Date(df.ad$test_date) < as.Date('2014-12-01'),]
df.ad.dec <- df.ad[as.Date(df.ad$test_date) >= as.Date('2014-12-01') & as.Date(df.ad$test_date) < as.Date('2015-01-01'),]

# plot grid
par(mfrow=c(3,1),mar=c(1,1,1,1),mai=c(0.1,1,0.1,0.1)) 

tbl <- table(df.ad.oct$device_id)
tbl <- sort(tbl[tbl>10]) # only keep > 10
nms <- rep("",dim(tbl))
barplot(tbl, names.arg = nms,main="October",ylab="Number of tests",xlab="Unique devices")
tbl.oct <- tbl

tbl <- table(df.ad.nov$device_id)
tbl <- sort(tbl[tbl>10]) # only keep > 10
nms <- rep("",dim(tbl))
barplot(tbl, names.arg = nms,main="November",ylab="Number of tests",xlab="Unique devices")
tbl.nov <- tbl

tbl <- table(df.ad.dec$device_id)
tbl <- sort(tbl[tbl>10]) # only keep > 10
nms <- rep("",dim(tbl))
barplot(tbl, names.arg = nms,main="December",ylab="Number of tests",xlab="Unique devices")
tbl.dec <- tbl

# max number of test to call a device suspicious
max_susp_dev = 30
```

So we remove specific devices that test more then `r max_susp_dev` times a month. These devices are probably used by telecom proffesionals for testing purposes. A test on 4G cost up to 100MBytes per test out of the data bundle(depending on the exact speed of course!). So 30 or more tests per month are equivalent to 3GB of data just spend on tests are suspicious. The number 30 by itself is subjective, we could use 25 or 40 depending on what exact level of tests per device you would call suspicious.

Actually including these are not influencing the results significantly, but we want to use real customer data as much as possible, not affected by proffesionals testing the own network.

So we exclude `r sum(tbl>=max_susp_dev)` specific devices based on Ookla *device_id*.

Cleaning up the data
-------------------
The SQl script to clean up the data maps duplicates or spelling issues for the telecom operator names. Also we do some transformations on the raw connection codes so we can identify technology used properly.

```{r get-clean-data}
# read data from 4Gdb
# Establish connection to PoststgreSQL using RPostgreSQL
drv <- dbDriver("PostgreSQL")
# Full version of connection setting
con <- dbConnect(drv, dbname="4Gdb",host="localhost",port=5432,user="4Guser",password="4Gpassword" )

# all data joined
df.clnd <- dbReadTable(con, c("ookla_all_data_clean"))

# get tm coverage data
df.4G <- dbReadTable(con, c("datatm4gcoverage"))

# disconnect
dbDisconnect(con)
```

So after removing `r sum(tbl.oct>=max_susp_dev)` devices in October, `r sum(tbl.nov>=max_susp_dev)` devices in November and`r sum(tbl.dec>=max_susp_dev)` devices in December, which in total conducted `r nrow(df.ad)-nrow(df.clnd)` speedtests, the cleaned data has `r nrow(df.clnd)` cases.

Let's do some more basic stats on this data.

Type of technology used:
```{r tech}
library(knitr)
tbl <- table(df.clnd$technology)
kable(as.data.frame(cbind(tbl,100*prop.table(tbl))),col.names = c('count','percentage'), digits = 2, caption="Type of technology used in tests")
```

In the remainder of this analysis we will focus on 4G technology.

Ookla Connetiontype defines 4G as connection type 15.

Defenition of 4G from the SQL script:

```sql
  Case  WHEN	CONNECTION_TYPE=0			THEN 'UNKNOWN'
  	WHEN	CONNECTION_TYPE in (1,2)		THEN 'WIFI/CELL'
  	WHEN	CONNECTION_TYPE in (3,4)		THEN '2G'
  	WHEN	CONNECTION_TYPE=15			THEN '4G'
  	WHEN	CONNECTION_TYPE between 5 and 17	THEN '3G'
  	ELSE	'UNKNOWN'
  END AS TECHNOLOGY
```
Top three operators
-------------------
For this analysis we are only interested in the top three operators in the Netherlands. In the raw data at this point there are `r length(levels(as.factor(df.clnd$operator)))` different operators identified. 
```{r top-operators}
t10 <- head(sort(table(df.clnd$operator),decreasing = TRUE),n=10)
kable(as.data.frame(t10))
```

The top three operators together are good for `r sum(tail(sort(table(df.clnd$operator)),n=3))` tests conducted all over the Netherlands in the test period.

Geographical area
------------------
For this test to be fair to all three operators we limit the comparison of the test to areas that all operators claim to have 4G coverages at the time of the measurements. The 4G network is constantly improved and extended, every month the area T-Mobile has 4G coverage is extended. This means that some areas only got 4G coverage during the time period of the test. 

Online [here](http://www.4gdekking.nl/) we get an up to date overview of network coverage for all three operators.

This test is not about coverage but about speed of the network. 

T-Mobile has the least 4G coverage of the three and per end of 2014 has actual coverage in the following area:

```{r area }
library(png)
library(grid)
img <- readPNG("./img/tmobile4gcoverage.png")
grid.raster(img)

```

In this area the following numbers remain
```{r}
tbl <- table(df.4G$operator)
kable(as.data.frame(cbind(tbl,100*prop.table(tbl))),col.names = c('count','percentage'), digits = 2,caption="Number of 4G speedtests in coverage area")
```

Areas that only had 4G coverage from November 2014 onwards are: Apeldoorn, Voorst, Dinkelland, Enschede, Losser, Oldenzaal. 
Areas that were hooked up in December are : Eijsden, Maastricht, Margraten, Bedum, Groningen, Haren, Hoogezand-sappemeer, Leek, Loppersum, Noordenveld, Slochteren, Ten Boer, Tynaarlo

### Suspicious speeds

Download speeds 4G are limited to 150Mbps on the T-mobile technology.

KPN and Vodafone have a technology called LT advanced which have a max of 225Mbps.
 
```{r susp-down-speed}
# count number of cases that exceed theoretical max speed
n.max.tm <- nrow(subset(df.4G, download_kbps>150000 & operator == "T-Mobile NL"))
n.max.vf <- nrow(subset(df.4G, download_kbps>225000 & operator == "Vodafone NL"))
n.max.kpn <-nrow(subset(df.4G, download_kbps>225000 & operator == "KPN NL"))

# create filters 
rw.fltr.1 <- rownames(subset(df.4G, download_kbps>150000 & operator == "T-Mobile NL"))
rw.fltr.2 <- rownames(subset(df.4G, download_kbps>225000 & operator == "Vodafone NL"))
rw.fltr.3 <- rownames(subset(df.4G, download_kbps>225000 & operator == "KPN NL"))

# filter out these extremes
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.1),]
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.2),]
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.3),]

```

So we remove suspicious measurements in which the **download** speed exceeded the maximum theoretical speed per individual operator. For T-Mobile we removed `r n.max.tm` cases, for Vodafone we removed `r n.max.vf` cases and for KPN we removed `r n.max.kpn` cases.

Let's do the same for uploads.

The maximum theoretical upload speed is for all operators the same: max 50Mbps.
```{r susp-upload-speed}
# create filters 
rw.fltr.4 <- rownames(subset(df.4G, upload_kbps>50000 & operator == "T-Mobile NL"))
rw.fltr.5 <-rownames(subset(df.4G, upload_kbps>50000 & operator == "Vodafone NL"))
rw.fltr.6 <-rownames(subset(df.4G, upload_kbps>50000 & operator == "KPN NL"))

# filter out these extremes
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.4),]
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.5),]
df.4G <- df.4G[setdiff(rownames(df.4G),rw.fltr.6),]

```

So again we remove suspicious measurements in which the **upload** speed exceeded the maximum theoretical speed per individual operator. For T-Mobile we removed `r length(rw.fltr.4)` cases, for Vodafone we removed `r length(rw.fltr.5)` cases and for KPN we removed `r length(rw.fltr.6)` cases.


### Suspicious coordinates (Hotspots)

Are there any locations, or coordinates, that occur very often in the area?
If we join the coordinates latitude and longitude together and look at the most frequent occurences we see that there are indeed some coordinates that are very frequent. Whenever the exact coordinates are not available, due to measurement issues or because the customer is not allowing the application to use the GPS coordinates Ookla uses GEO-IP.

```{r susp-coord}

# combine coordinates in "lat-lon" 
str.coord <- paste(df.4G$client_latitude,",",df.4G$client_longitude)

# top 10 most frequent locations
tbl <- head(sort(table(str.coord),decreasing = TRUE),n=15)
nms <- names(tbl)
df.tmp <- as.data.frame(cbind(nms,tbl))
names(df.tmp) <- c("Coordinates","Count")
kable(df.tmp, row.names = FALSE)

coord <- names(sort(table(str.coord),decreasing = TRUE)[1])
n.test.at.coord <- sort(table(str.coord),decreasing = TRUE)[1]

```

To understand more on how GEO-IP works we asked some specific questions to Ookla on these issues.

Coordinate (52.5-5.75) : Response from ookla: *"The IP addresses associated with these tests show up as in the Netherlands, but that's specific as it gets regarding GEO-IP. Were assuming that 52.5, 5.75 is the Dutch equivalent of the middle of Kansas (in the US if GEO-IP Cannot be determined it defaults to the center of the US which in our case is Kansas). All it knows is it's in the Netherlands, so GeoIP specifies the center of the country."*

Coordinates (52.3667-4.9) and (52.35-4.9167) are both in Amsterdam, but in one case the latidude is with less precision and in the other the longitude. Question to Ookla: Can we still asume the measurements are from Amsterdam? 
Response Ookla: *Yes, GeoIP is used here. We can assume you are in Amsterdam but like any other GeoIP location result, the confidence level isn't as high as it would be if we were able to get location information directly from the device, meaning if we were able to obtain GPS instead of GEO-IP.* 

Coordinate (51.9167-4.5) is in Rotterdam. Again with limited precision, same respoonse as above from Ookla.

Coordinate (52.0666-4.3209) is in The Hague. All cases are tests with operator equal to "T-Mobile NL" also this location is close to the T-Mobile office in The Hague. We will exclude these tests as potentially being from T-Mobile employees.

Also see the [precision of the coordinates](https://en.wikipedia.org/wiki/Decimal_degrees) denotes the fact that we are unsure about the exact location.

In the top location, which has coordinates (`r coord`) and is in the gemeente Dronten. We have `r n.test.at.coord` speedtests with this coordinate alone. We will analyse this set the same way we analyse individual Gemeentes.

```{r default-location}
# filter unkown location
df.unkwnloc <- df.4G[paste(df.4G$client_latitude,",",df.4G$client_longitude)==coord,]

# filter 52.0666-4.3209
coord <- "52.0666 , 4.3209"
df.dh <- df.4G[paste(df.4G$client_latitude,",",df.4G$client_longitude)==coord,]

# filter unkown location from 4G as we want only known locations
df.4G <- df.4G[setdiff(rownames(df.4G),rownames(df.dh)),]

# replace Dronten with UNKNOWN
df.unkwnloc$gm_naam <- "Unkown Location"

# filter unkown location from 4G as we want only known locations
df.4G <- df.4G[setdiff(rownames(df.4G),rownames(df.unkwnloc)),]

```

We removed `r nrow(df.dh)` tests from coordinates "52.0666 , 4.3209" in The Hague.

### Suspicious dates or times

Again are there any suspicious peaks in dates or times in the data?
```{r timestamps, fig.align='center'}

#Timeseries
counts <- table(df.4G$operator, df.4G$test_date)

barplot(counts,
  xlab="Dates", col=c("green","magenta","red"))
legend("topright", 
       legend = rownames(counts), 
       fill = c("green","magenta","red"), 
       cex = 0.5)

```


\newpage

Speedtest
=============
So we have analysed the data and looked for anomallies in the data, if found we have corrected them. Finally we are ready to compare speed test data between the three major telecom operators in Netherlands in the above defined coverage area. 

We  will analyse three different metrics:

* Download speed
* Upload speed
* Latency

So how are the different metrics distributed?

Histogram distributions
----------------------
A histogram is a graphical representation of the distribution of data. It is an estimate of the probability distribution of a continuous variable (quantitative variable), [more](https://en.wikipedia.org/wiki/Histogram) about histograms on wikipedia.

### Download speed

```{r downloadspeed, fig.cap='Histogram download speed per operator', fig.height=3, fig.pos='H' }
library(plyr)
cdf <- ddply(df.4G, "operator", summarise, download_kbps.mean=mean(download_kbps))

# With mean lines, using cdf from above
ggplot(df.4G, aes(x=download_kbps)) + geom_histogram( colour="black", fill="white") + 
    facet_grid(operator ~ .) +
    geom_vline(data=cdf, aes(xintercept=download_kbps.mean),
               linetype="solid", size=1, colour="red")

```


### Upload speed

```{r uploadspeed, fig.cap='Histogram upload speed per operator', fig.height=3 }
library(plyr)
cdf <- ddply(df.4G, "operator", summarise, upload_kbps.mean=mean(upload_kbps))

# With mean lines, using cdf from above
ggplot(df.4G, aes(x=upload_kbps)) + geom_histogram( colour="black", fill="white") + 
    facet_grid(operator ~ .) +
    geom_vline(data=cdf, aes(xintercept=upload_kbps.mean),
               linetype="solid", size=1, colour="red")

```

### Latency
For latency we take the log so the outliers scale and we can have a look at the shape of the distribution.

```{r latency, fig.cap='Histogram latency per operator', fig.height=3 }
library(plyr)
cdf <- ddply(df.4G, "operator", summarise, latency.mean=mean(log(latency)))

# With mean lines, using cdf from above
ggplot(df.4G, aes(x=log(latency))) + geom_histogram( colour="black", fill="white") + 
    facet_grid(operator ~ .) #+
#     geom_vline(data=cdf, aes(xintercept=log(latency.mean)),
#                linetype="dashed", size=1, colour="red")

```

Boxplot
---------
In descriptive statistics, a box plot or boxplot is a convenient way of graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. [More](https://en.wikipedia.org/wiki/Box_plot) about boxplots on wikipedia.

### Download speed

```{r box-down, fig.cap='Boxplot download speed per operator'}
# boxplot
ggplot(df.4G, aes(x=operator, y=download_kbps, fill=operator)) + geom_boxplot() +
    guides(fill=FALSE) + stat_summary(fun.y=mean, geom="point", shape=5, size=4)

```

### Upload speed

```{r box-up, fig.cap='Boxplot upload speed per operator'}
# boxplot
ggplot(df.4G, aes(x=operator, y=upload_kbps, fill=operator)) + geom_boxplot() +
    guides(fill=FALSE) + stat_summary(fun.y=mean, geom="point", shape=5, size=4)

```

### Latency
For latency ( or ping) we take the log so the outliers scale and we can have a look at the shape of the distribution.
```{r box-latency, fig.cap='Boxplot latency speed per operator'}
# boxplot
ggplot(df.4G, aes(x=operator, y=log(latency), fill=operator)) + geom_boxplot() +
    guides(fill=FALSE) + stat_summary(fun.y=mean, geom="point", shape=5, size=4)

```

Significance
-----------
In practice, the Central Limit Theorem assures us that, under a wide range of assumptions, the distributions of the two sample means being tested will themselves approach Normal distributions as the sample sizes get large, regardless (this is where the assumptions come in) of the distributions of the underlying data. As a consequence, as the sample size gets larger, the difference of the means becomes normally distributed, and the requirements necessary for the t-statistic of an unpaired t-test to have the nominal t distribution become satisfied.

Which test?
-----------
What we have here is a set of unpaired, independent, different samplesize, different variance data. So we need a [Welch t-test](https://en.wikipedia.org/wiki/Welch%27s_t_test).

P-value
---------
In statistics, the p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before performing the test a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% [1] and denoted as $\alpha$. If the p-value is equal or smaller than the significance level ($\alpha$), it suggests that the observed data is inconsistent with the assumption that the null hypothesis is true, and thus that hypothesis must be rejected and the alternative hypothesis is accepted as true ( see [wikipedia](https://en.wikipedia.org/wiki/P-value)).

```{r set-confidence-level}
conflevel <- 0.95
```


Test results
===========
We test each operator agains the other two operators so we have three tests. We put the confidence level to `r 100*conflevel` %. Our null-hypothesis is that the means are drawn from the same sample, so they are not different.

Let see what our test results are:

```{r absolute-test-result}
# do test result use sourced function
df.tr.dwnload <- fn.ttest(df.4G,"download_kbps")
df.tr.upload <- fn.ttest(df.4G,"upload_kbps")
df.tr.latency <- fn.ttest(df.4G,"latency")

# print test result 
kable(df.tr.dwnload,format = "pandoc" ,  caption = "Absolute comparison of means for metric: download(kbps)",align='c')
kable(df.tr.upload,format = "pandoc" ,  caption = "Absolute comparison of means for metric: upload(kbps)",align='c')
kable(df.tr.latency,format = "pandoc",  caption = "Absolute comparison of means for metric: latency",align='c')

```

Looking at the tables above we see that all results are significant and the P-values are very small. This means we can reject the null-hypothesis with great confidence. Hence we can state that the differences in the mean are significant.

Another way of looking at this result is that we can look at the lower bound of the confidence interval of the difference in the mean.
```{r relative conslusions}
# sometimes R is a bit cumbersome...
min.diff.tmvskpn <- as.numeric(as.character(df.tr.dwnload["T-Mobile vs KPN","Lower"]))
num.mean.kpn <- as.numeric(as.character(df.tr.dwnload["T-Mobile vs KPN","Mean 2"]))

min.diff.tmvsvf <- as.numeric(as.character(df.tr.dwnload["T-Mobile vs Vodafone","Lower"]))
num.mean.vf <- as.numeric(as.character(df.tr.dwnload["T-Mobile vs Vodafone","Mean 2"]))

```

So if one would like to make a relative statement about these testresults this could be done like the following:
With `r 100*conflevel`% certainty we can state that in the investigated area the 4G network of T-Mobile outperforms KPN by at least `r round(min.diff.tmvskpn/1000,2)`Mbps, which is at least `r round(100*(min.diff.tmvskpn/num.mean.kpn),1)`% faster.
Similarly we can say that with `r 100*conflevel`% certainty we can state that in the investigated area the 4G network of T-Mobile outperforms Vodafone by at least `r round(min.diff.tmvsvf/1000,2)`Mbps, which is `r round(100*(min.diff.tmvsvf/num.mean.vf),1)`% faster.

Similar statements can be derived from the result tables per individual gemeente below. Remark: Most, but not all tests at the Gemeente level are significant.

Conclusion
----------
This analysis has been conducted with the utmost care and to the best knowledge of the analyst. The analysis is opensource and all code can be downloaded, reviewed and repeated from [GitHub](https://github.com/hugokoopmans/ookla-speedtest-analysis).

Overall the tests performed give a significant result. Per gemeente some tests were not significant.

\newpage

Top 20 Gemeentes 
=============
From CBS we have the following top 20 gemeentes based on number of inhabitants("aantal inwoners"). 


```{r gettop20}

# postgreSQL
require("RPostgreSQL", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.0")

# Establish connection to PoststgreSQL using RPostgreSQL
drv <- dbDriver("PostgreSQL")

# Full version of connection setting
con <- dbConnect(drv, dbname="4Gdb",host="localhost",port=5432,user="4Guser",password="4Gpassword" )

# all data joined on top 20 gemeentes
res <- dbSendQuery(con, "SELECT gm_code,gm_naam,aantal_inwoners from top20layer")

df.top20 <- fetch(res, n = -1)

# disconnect
dbDisconnect(con)

# print table
kable(df.top20)

# Add unknown as seperate "gemeente"
df.top20.plus.unknown <- rbind(df.top20, c("---","Unkown Location", nrow(df.unkwnloc)))

# add data of unknown to df
df.4G.plus.hotspots <- rbind(df.4G,df.unkwnloc)

```

In this test we use the CBS gemeentes in this area as the benchmark area for the overall comparison.

In more detailed analysis we investigate the top twenty "Gemeentes" based on number of inhabitants. based on data available at [CBS](http://www.cbs.nl/nl-NL/menu/themas/dossiers/nederland-regionaal/publicaties/geografische-data/archief/2014/2013-wijk-en-buurtkaart-art.htm).

The coverage area of top 20 gemeentes looks like this.

```{r top20, echo=FALSE}
library(png)
library(grid)
img <- readPNG("./img/top20gemeentes.png")
 grid.raster(img)
```

From the CBS data we learn that the top 20 gemeentes covers 4855450 out of 16779185 inhabitants. Which is `r round(100*4855450/16779185,2)` %. in terms of area this is 2412 km^2 of a total of 41545 km^2, which is `r round(100*2412/41545,2)` %. 

For each gemeente we will do the significance test seperatly in the next pages.

```{r run-analysis-per-gemeente, comment='', echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE, results='asis'}
# run the analysis for each gemeente in the df.top20
out = NULL
for (i in  1:nrow(df.top20.plus.unknown)) {
#  out = c(out, knit_expand('analysis-per-gemeente.Rmd'))
  out = c(out, knit_child('analysis-per-gemeente.Rmd'))
}

cat(paste(out, collapse = '\n'))
#cat(knit(text=unlist(paste(out, collapse = '\n')), quiet=TRUE))

```





